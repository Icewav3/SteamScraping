{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d82f6d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiohttp in c:\\users\\coled\\anaconda3\\envs\\comp2040\\lib\\site-packages (3.11.16)\n",
      "Requirement already satisfied: nest_asyncio in c:\\users\\coled\\anaconda3\\envs\\comp2040\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\coled\\anaconda3\\envs\\comp2040\\lib\\site-packages (from aiohttp) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\coled\\anaconda3\\envs\\comp2040\\lib\\site-packages (from aiohttp) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\coled\\anaconda3\\envs\\comp2040\\lib\\site-packages (from aiohttp) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\coled\\anaconda3\\envs\\comp2040\\lib\\site-packages (from aiohttp) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\coled\\anaconda3\\envs\\comp2040\\lib\\site-packages (from aiohttp) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\coled\\anaconda3\\envs\\comp2040\\lib\\site-packages (from aiohttp) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\coled\\anaconda3\\envs\\comp2040\\lib\\site-packages (from aiohttp) (1.19.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\coled\\anaconda3\\envs\\comp2040\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.10)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\coled\\anaconda3\\envs\\comp2040\\lib\\site-packages (8.1.6)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\coled\\anaconda3\\envs\\comp2040\\lib\\site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\coled\\anaconda3\\envs\\comp2040\\lib\\site-packages (from ipywidgets) (8.30.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\coled\\anaconda3\\envs\\comp2040\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in c:\\users\\coled\\anaconda3\\envs\\comp2040\\lib\\site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.14 in c:\\users\\coled\\anaconda3\\envs\\comp2040\\lib\\site-packages (from ipywidgets) (3.0.14)\n",
      "Requirement already satisfied: decorator in c:\\users\\coled\\anaconda3\\envs\\comp2040\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\coled\\anaconda3\\envs\\comp2040\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\coled\\anaconda3\\envs\\comp2040\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\coled\\anaconda3\\envs\\comp2040\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\coled\\anaconda3\\envs\\comp2040\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\coled\\anaconda3\\envs\\comp2040\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in c:\\users\\coled\\anaconda3\\envs\\comp2040\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\coled\\anaconda3\\envs\\comp2040\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\coled\\anaconda3\\envs\\comp2040\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\coled\\anaconda3\\envs\\comp2040\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: executing in c:\\users\\coled\\anaconda3\\envs\\comp2040\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\coled\\anaconda3\\envs\\comp2040\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\coled\\anaconda3\\envs\\comp2040\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install aiohttp nest_asyncio\n",
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c64ea9",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3639d6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from tqdm.notebook import tqdm\n",
    "import nest_asyncio\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15e8e18",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d503221",
   "metadata": {},
   "source": [
    "Amount of pages to scrape from steamspy (each page is ~1000 games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7abef0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEAMSPY_PAGES = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3babeb43",
   "metadata": {},
   "source": [
    "# Setting up data folder to save to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f7ed2eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"Data\"\n",
    "TODAY_DATE = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "DAILY_DATA_DIR = os.path.join(DATA_DIR, TODAY_DATE)\n",
    "os.makedirs(DAILY_DATA_DIR, exist_ok=True)\n",
    "\n",
    "OUTPUT_FILE = os.path.join(DAILY_DATA_DIR, \"steamspy_data.jsonl\")\n",
    "PROGRESS_LOG = os.path.join(DAILY_DATA_DIR, \"scraped_appids.txt\")\n",
    "ERROR_LOG = os.path.join(DAILY_DATA_DIR, \"steamspy_errors.log\")\n",
    "METADATA_FILE = os.path.join(DAILY_DATA_DIR, \"daily_metadata.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd683936",
   "metadata": {},
   "source": [
    "## Async setup & steamspy definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cd520851",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nest_asyncio.apply()\n",
    "BASE_URL = \"https://steamspy.com/api.php\"\n",
    "ALL_REQUEST_DELAY = 60  # seconds between 'all' page requests\n",
    "APPDETAILS_RATE_INTERVAL = 1.0  # seconds between appdetails requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9695aa0",
   "metadata": {},
   "source": [
    "# Async class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a32b304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RateLimiter:\n",
    "    def __init__(self, interval):\n",
    "        self.interval = interval\n",
    "        self.lock = asyncio.Lock()\n",
    "        self.last_called = 0\n",
    "\n",
    "    async def __aenter__(self):\n",
    "        async with self.lock:\n",
    "            now = asyncio.get_event_loop().time()\n",
    "            # Calculate if we need to wait\n",
    "            wait_time = self.interval - (now - self.last_called)\n",
    "            if wait_time > 0:\n",
    "                await asyncio.sleep(wait_time)\n",
    "            self.last_called = asyncio.get_event_loop().time()\n",
    "\n",
    "    async def __aexit__(self, exc_type, exc, tb):\n",
    "        pass\n",
    "\n",
    "# Create a global rate limiter for app details requests\n",
    "appdetails_rate_limiter = RateLimiter(APPDETAILS_RATE_INTERVAL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5cd315",
   "metadata": {},
   "source": [
    "# Logging functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9234130e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_error(message):\n",
    "    \"\"\"Append error message to the error log file.\"\"\"\n",
    "    with open(ERROR_LOG, 'a', encoding='utf-8') as f:\n",
    "        f.write(message + '\\n')\n",
    "\n",
    "def save_progress(appid):\n",
    "    \"\"\"Append a successfully scraped appid to the progress log.\"\"\"\n",
    "    with open(PROGRESS_LOG, 'a', encoding='utf-8') as f:\n",
    "        f.write(str(appid) + '\\n')\n",
    "\n",
    "def load_scraped_ids():\n",
    "    \"\"\"Load all appids that have been scraped already.\"\"\"\n",
    "    if os.path.exists(PROGRESS_LOG):\n",
    "        with open(PROGRESS_LOG, 'r', encoding='utf-8') as f:\n",
    "            return set(int(line.strip()) for line in f if line.strip())\n",
    "    return set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb620922",
   "metadata": {},
   "source": [
    "# Metadata Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9eec38b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_daily_metadata(start_time, end_time, attempted_count, successful_count, failed_count, error_log_path, metadata_file_path, max_pages):\n",
    "    \"\"\"Saves the daily metadata to a JSON file.\"\"\"\n",
    "    TODAY_DATE = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    error_counts = {\"connection_error\": 0, \"timeout_error\": 0, \"other_errors\": 0}\n",
    "    if os.path.exists(error_log_path):\n",
    "        with open(error_log_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if \"HTTP\" in line.lower() or \"connection\" in line.lower():\n",
    "                    error_counts[\"connection_error\"] += 1\n",
    "                elif \"timeout\" in line.lower():\n",
    "                    error_counts[\"timeout_error\"] += 1\n",
    "                else:\n",
    "                    error_counts[\"other_errors\"] += 1\n",
    "\n",
    "    metadata = {\n",
    "        \"scrape_date\": TODAY_DATE,\n",
    "        \"scrape_start_time\": start_time,\n",
    "        \"scrape_end_time\": end_time,\n",
    "        \"total_appids_attempted\": attempted_count,\n",
    "        \"total_appids_successful\": successful_count,\n",
    "        \"total_appids_failed\": failed_count,\n",
    "        \"error_summary\": error_counts,\n",
    "        \"api_endpoints_used\": [BASE_URL + \"?request=all\", BASE_URL + \"?request=appdetails&appid=\"],\n",
    "        \"scrape_parameters\": {\"max_pages\": max_pages},\n",
    "        \"scraper_version\": \"1.0\", # Update if you make changes\n",
    "        \"notes\": \"Daily rescrape of all available pages.\"\n",
    "    }\n",
    "\n",
    "    with open(metadata_file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "\n",
    "    print(f\"Metadata saved to: {metadata_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490b389d",
   "metadata": {},
   "source": [
    "# Async querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2b2c2739",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_all_apps(session, page=0):\n",
    "    \"\"\"Fetch the 'all' endpoint which returns a list of apps for a given page.\"\"\"\n",
    "    params = {\"request\": \"all\", \"page\": page}\n",
    "    try:\n",
    "        async with session.get(BASE_URL, params=params) as response:\n",
    "            if response.status != 200:\n",
    "                log_error(f\"Error on all page {page}: HTTP {response.status}\")\n",
    "                return {}\n",
    "            return await response.json()\n",
    "    except Exception as e:\n",
    "        log_error(f\"Error on all page {page}: {e}\")\n",
    "        return {}\n",
    "\n",
    "async def fetch_app_details(session, appid):\n",
    "    \"\"\"Fetch details for a single app using the rate limiter.\"\"\"\n",
    "    params = {\"request\": \"appdetails\", \"appid\": appid}\n",
    "    \n",
    "    # Wait for token before doing the request.\n",
    "    async with appdetails_rate_limiter:\n",
    "        try:\n",
    "            async with session.get(BASE_URL, params=params) as response:\n",
    "                if response.status != 200:\n",
    "                    log_error(f\"App {appid}: HTTP {response.status}\")\n",
    "                    return None\n",
    "                data = await response.json()\n",
    "                # Filter out if the developer has hidden the data\n",
    "                if data.get(\"appid\") == 999999:\n",
    "                    return None\n",
    "                return data\n",
    "        except Exception as e:\n",
    "            log_error(f\"App {appid}: {e}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7361658c",
   "metadata": {},
   "source": [
    "# Scraping data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cacc02d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_appdetails_for_list(session, app_ids):\n",
    "    \"\"\"\n",
    "    For each appid in app_ids (skipping already scraped ones),\n",
    "    scrape app details and append data to the output file.\n",
    "    Returns the count of new apps scraped.\n",
    "    \"\"\"\n",
    "    scraped_ids = load_scraped_ids()\n",
    "    new_scraped = 0\n",
    "\n",
    "    # Open output file in append mode\n",
    "    with open(OUTPUT_FILE, 'a', encoding='utf-8') as f:\n",
    "        # Use tqdm to monitor progress\n",
    "        for appid in tqdm(app_ids, desc=\"Scraping appdetails\"):\n",
    "            # Skip if already processed\n",
    "            if appid in scraped_ids:\n",
    "                continue\n",
    "            data = await fetch_app_details(session, appid)\n",
    "            if data:\n",
    "                json.dump(data, f)\n",
    "                f.write('\\n')\n",
    "                save_progress(appid)\n",
    "                new_scraped += 1\n",
    "    return new_scraped\n",
    "\n",
    "async def scrape_all(max_pages=2):\n",
    "    \"\"\"\n",
    "    Main async function that iterates over pages of 'all' endpoints,\n",
    "    gathers app ids, and calls app details scraper.\n",
    "    Respects the 60-second delay between consecutive 'all' page requests.\n",
    "    Handles the core scraping logic and calls the metadata saving function.\n",
    "    \"\"\"\n",
    "    start_time = datetime.now().isoformat()\n",
    "    total_attempted = 0\n",
    "    total_successful = 0\n",
    "    total_failed = 0\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for page in range(max_pages):\n",
    "            print(f\"\\nFetching app list from page {page}...\")\n",
    "            apps = await get_all_apps(session, page=page)\n",
    "            if not apps:\n",
    "                print(f\"Warning: No apps returned for page {page}\")\n",
    "                continue\n",
    "\n",
    "            # Extract app IDs from keys of the JSON result\n",
    "            app_ids = [int(appid) for appid in apps.keys()]\n",
    "            attempted_on_page = len(app_ids)\n",
    "            successful_on_page = await scrape_appdetails_for_list(session, app_ids)\n",
    "            failed_on_page = attempted_on_page - successful_on_page\n",
    "            print(f\"Page {page} - Attempted: {attempted_on_page}, Successful: {successful_on_page}, Failed: {failed_on_page}\")\n",
    "\n",
    "            total_attempted += attempted_on_page\n",
    "            total_successful += successful_on_page\n",
    "            total_failed += failed_on_page\n",
    "\n",
    "            # Wait before fetching the next page\n",
    "            if page < max_pages - 1:\n",
    "                print(\"Waiting 60 seconds before fetching next page...\")\n",
    "                await asyncio.sleep(ALL_REQUEST_DELAY)\n",
    "\n",
    "    end_time = datetime.now().isoformat()\n",
    "\n",
    "    # Save the daily metadata\n",
    "    save_daily_metadata(start_time, end_time, total_attempted, total_successful, total_failed, ERROR_LOG, METADATA_FILE, max_pages)\n",
    "\n",
    "    print(f\"\\nScraping complete for {TODAY_DATE}.\")\n",
    "    print(f\"Data saved to: {OUTPUT_FILE}\")\n",
    "    print(f\"Errors logged in: {ERROR_LOG}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e34ca8",
   "metadata": {},
   "source": [
    "Scraping time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fe8c90c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching app list from page 0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21599d625e804f7bafae2ca62cb5a4ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scraping appdetails:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 0 - Attempted: 1000, Successful: 2, Failed: 998\n",
      "Metadata saved to: Data\\2025-05-19\\daily_metadata.json\n",
      "\n",
      "Scraping complete for 2025-05-19.\n",
      "Data saved to: Data\\2025-05-19\\steamspy_data.jsonl\n",
      "Errors logged in: Data\\2025-05-19\\steamspy_errors.log\n"
     ]
    }
   ],
   "source": [
    "await scrape_all(max_pages=STEAMSPY_PAGES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp2040",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
