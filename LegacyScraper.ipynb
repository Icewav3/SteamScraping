{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82f6d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install aiohttp nest_asyncio\n",
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c64ea9",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3639d6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from tqdm.notebook import tqdm\n",
    "import nest_asyncio\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15e8e18",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d503221",
   "metadata": {},
   "source": [
    "Amount of pages to scrape from steamspy (each page is ~1000 games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abef0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEAMSPY_PAGES = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3babeb43",
   "metadata": {},
   "source": [
    "# Setting up data folder to save to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ed2eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"Data\"\n",
    "TODAY_DATE = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "DAILY_DATA_DIR = os.path.join(DATA_DIR, TODAY_DATE)\n",
    "os.makedirs(DAILY_DATA_DIR, exist_ok=True)\n",
    "\n",
    "OUTPUT_FILE = os.path.join(DAILY_DATA_DIR, \"steamspy_data.jsonl\")\n",
    "PROGRESS_LOG = os.path.join(DAILY_DATA_DIR, \"scraped_appids.txt\")\n",
    "ERROR_LOG = os.path.join(DAILY_DATA_DIR, \"steamspy_errors.log\")\n",
    "META_FILE = os.path.join(DAILY_DATA_DIR, \"metadata.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd683936",
   "metadata": {},
   "source": [
    "## Async setup & steamspy definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd520851",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nest_asyncio.apply()\n",
    "BASE_URL = \"https://steamspy.com/api.php\"\n",
    "ALL_REQUEST_DELAY = 1  # seconds between 'all' page requests\n",
    "APPDETAILS_RATE_INTERVAL = 0.1  # seconds between appdetails requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9695aa0",
   "metadata": {},
   "source": [
    "# Async class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32b304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RateLimiter:\n",
    "    def __init__(self, interval):\n",
    "        self.interval = interval\n",
    "        self.lock = asyncio.Lock()\n",
    "        self.last_called = 0\n",
    "\n",
    "    async def __aenter__(self):\n",
    "        async with self.lock:\n",
    "            now = asyncio.get_event_loop().time()\n",
    "            # Calculate if we need to wait\n",
    "            wait_time = self.interval - (now - self.last_called)\n",
    "            if wait_time > 0:\n",
    "                await asyncio.sleep(wait_time)\n",
    "            self.last_called = asyncio.get_event_loop().time()\n",
    "\n",
    "    async def __aexit__(self, exc_type, exc, tb):\n",
    "        pass\n",
    "\n",
    "# Create a global rate limiter for app details requests\n",
    "appdetails_rate_limiter = RateLimiter(APPDETAILS_RATE_INTERVAL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5cd315",
   "metadata": {},
   "source": [
    "# Logging functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9234130e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_error(message):\n",
    "    \"\"\"Append error message to the error log file.\"\"\"\n",
    "    with open(ERROR_LOG, 'a', encoding='utf-8') as f:\n",
    "        f.write(message + '\\n')\n",
    "\n",
    "def save_progress(appid):\n",
    "    \"\"\"Append a successfully scraped appid to the progress log.\"\"\"\n",
    "    with open(PROGRESS_LOG, 'a', encoding='utf-8') as f:\n",
    "        f.write(str(appid) + '\\n')\n",
    "\n",
    "def load_scraped_ids():\n",
    "    \"\"\"Load all appids that have been scraped already.\"\"\"\n",
    "    if os.path.exists(PROGRESS_LOG):\n",
    "        with open(PROGRESS_LOG, 'r', encoding='utf-8') as f:\n",
    "            return set(int(line.strip()) for line in f if line.strip())\n",
    "    return set()\n",
    "def save_metadata(metadata):\n",
    "    \"\"\"Save metadata to a JSON file.\"\"\"\n",
    "    with open(META_FILE, 'w', encoding='utf-8') as f:\n",
    "        json.dump(metadata, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490b389d",
   "metadata": {},
   "source": [
    "# Async querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2c2739",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_all_apps(session, page=0):\n",
    "    \"\"\"Fetch the 'all' endpoint which returns a list of apps for a given page.\"\"\"\n",
    "    params = {\"request\": \"all\", \"page\": page}\n",
    "    try:\n",
    "        async with session.get(BASE_URL, params=params) as response:\n",
    "            if response.status != 200:\n",
    "                log_error(f\"Error on all page {page}: HTTP {response.status}\")\n",
    "                return {}\n",
    "            return await response.json()\n",
    "    except Exception as e:\n",
    "        log_error(f\"Error on all page {page}: {e}\")\n",
    "        return {}\n",
    "\n",
    "async def fetch_app_details(session, appid):\n",
    "    \"\"\"Fetch details for a single app using the rate limiter.\"\"\"\n",
    "    params = {\"request\": \"appdetails\", \"appid\": appid}\n",
    "    \n",
    "    # Wait for token before doing the request.\n",
    "    async with appdetails_rate_limiter:\n",
    "        try:\n",
    "            async with session.get(BASE_URL, params=params) as response:\n",
    "                if response.status != 200:\n",
    "                    log_error(f\"App {appid}: HTTP {response.status}\")\n",
    "                    return None\n",
    "                data = await response.json()\n",
    "                # Filter out if the developer has hidden the data\n",
    "                if data.get(\"appid\") == 999999:\n",
    "                    return None\n",
    "                return data\n",
    "        except Exception as e:\n",
    "            log_error(f\"App {appid}: {e}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7361658c",
   "metadata": {},
   "source": [
    "# Scraping data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacc02d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_appdetails_for_list(session, app_ids):\n",
    "    \"\"\"\n",
    "    For each appid in app_ids (skipping already scraped ones),\n",
    "    scrape app details and append data to the output file.\n",
    "    Returns the count of new apps scraped.\n",
    "    \"\"\"\n",
    "    scraped_ids = load_scraped_ids()\n",
    "    new_scraped = 0\n",
    "\n",
    "    # Open output file in append mode\n",
    "    with open(OUTPUT_FILE, 'a', encoding='utf-8') as f:\n",
    "        # Use tqdm to monitor progress\n",
    "        for appid in tqdm(app_ids, desc=\"Scraping appdetails\"):\n",
    "            # Skip if already processed\n",
    "            if appid in scraped_ids:\n",
    "                continue\n",
    "            data = await fetch_app_details(session, appid)\n",
    "            if data:\n",
    "                json.dump(data, f)\n",
    "                f.write('\\n')\n",
    "                save_progress(appid)\n",
    "                new_scraped += 1\n",
    "    return new_scraped\n",
    "\n",
    "async def scrape_all(max_pages=2):\n",
    "    \"\"\"\n",
    "    Main async function that iterates over pages of 'all' endpoints,\n",
    "    gathers app ids, and calls app details scraper.\n",
    "    Respects the 60-second delay between consecutive 'all' page requests only if new data was scraped.\n",
    "    \"\"\"\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for page in range(max_pages):\n",
    "            print(f\"\\nFetching app list from page {page}...\")\n",
    "            apps = await get_all_apps(session, page=page)\n",
    "            if not apps:\n",
    "                print(f\"Warning: No apps returned for page {page}\")\n",
    "                continue\n",
    "\n",
    "            # Extract app IDs from keys of the JSON result\n",
    "            app_ids = [int(appid) for appid in apps.keys()]\n",
    "            new_count = await scrape_appdetails_for_list(session, app_ids)\n",
    "            print(f\"Page {page} - New apps scraped: {new_count}\")\n",
    "\n",
    "            # Only wait if new data was scraped (and it's not the last page)\n",
    "            if page < max_pages - 1:\n",
    "                if new_count > 0:\n",
    "                    print(\"Waiting 60 seconds before fetching next page...\")\n",
    "                    await asyncio.sleep(ALL_REQUEST_DELAY)\n",
    "                else:\n",
    "                    print(\"No new data scraped, skipping delay for next page.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e34ca8",
   "metadata": {},
   "source": [
    "Scraping time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8c90c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "await scrape_all(max_pages=STEAMSPY_PAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52b6574",
   "metadata": {},
   "source": [
    "## TODO Meta data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cd563f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo meta data json\n",
    "#save:\n",
    "#Parameters the program was ran with (amount of pages to scrape)\n",
    "#total unique appids scraped\n",
    "#exact date time started and finished\n",
    "#time and date for each start/stop if not done in one execution\n",
    "#total time taken to scrape\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp2040",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
