# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ® GAME MARKET DATA COLLECTION PIPELINE - Enhanced Main Orchestrator
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#
# CHANGES FROM ORIGINAL:
# - Line 42-47: Added max_backups configuration input
# - Line 65-67: New download-previous phase before scraping
# - Line 104-107: Pass max_backups to upload workflow
# - Line 129-166: Enhanced validation with data loss checks
# - Line 180-200: Comprehensive statistics in summary
#
# WHY: Orchestrates historical data preservation, validation gates, and
#      configurable backup retention in a single cohesive pipeline.
#
# Execution Order:
#   1. Download Phase (get previous release data)
#   2. Scrape Phase (parallel execution of all scrapers)
#   3. Compression Phase (merge historical + new data)
#   4. Validation Phase (check for data loss)
#   5. Upload Phase (publish to releases with backup rotation)
#
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

name: ğŸš€ Data Collection Pipeline

on:
  schedule:
    - cron: '0 16 * * *' # Daily at 8:00 AM PST
  
  workflow_dispatch:
    inputs:
      run_scrapers:
        description: 'Run scraping tasks'
        required: true
        default: true
        type: boolean
      
      steamspy_pages:
        description: 'SteamSpy pages to scrape'
        required: false
        default: '10'
        type: string
      
      steamspy_page_delay:
        description: 'Delay between SteamSpy page requests (seconds)'
        required: false
        default: '15'
        type: string

      steamspy_app_delay:
        description: 'Delay between SteamSpy app detail requests (seconds)'
        required: false
        default: '0.1'
        type: string

      max_backups:
        description: 'Maximum backup releases to keep (0 = unlimited)'
        required: false
        default: '30'
        type: string

      skip_upload:
        description: 'Skip upload phase (testing only)'
        required: false
        default: false
        type: boolean

env:
  DATA_DIR: 'Data'
  PYTHON_VERSION: '3.13'

jobs:
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # PHASE 0: DOWNLOAD PREVIOUS DATA
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  
  download-previous:
    name: ğŸ“¥ Download Previous Release
    uses: ./.github/workflows/download-previous.yml
    secrets: inherit
  
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # PHASE 1: SCRAPING (Parallel Execution)
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  
  scrape-steamspy:
    name: ğŸ® SteamSpy Scraper
    if: ${{ github.event.inputs.run_scrapers != 'false' }}
    needs: download-previous
    uses: ./.github/workflows/scrape-steamspy.yml
    secrets: inherit
    with:
      pages: ${{ github.event.inputs.steamspy_pages || '1000' }}
      page_delay: ${{ github.event.inputs.steamspy_page_delay || '15' }}
      app_delay: ${{ github.event.inputs.steamspy_app_delay || '0.1' }}
      test_mode: false
  
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # PHASE 2: COMPRESSION (Merge & Compress)
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  
  compress-data:
    name: ğŸ“¦ Compress Data
    needs: 
      - download-previous
      - scrape-steamspy
    uses: ./.github/workflows/compress.yml
    secrets: inherit
  
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # PHASE 3: VALIDATION (Data Integrity Checks)
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  
  validate-data:
    name: âœ… Validate Data Integrity
    needs: compress-data
    runs-on: ubuntu-latest
    timeout-minutes: 5
    
    outputs:
      validation_passed: ${{ steps.validate.outputs.passed }}
      has_warnings: ${{ steps.validate.outputs.has_warnings }}
    
    steps:
      - name: ğŸ“¥ Download Metadata
        uses: actions/download-artifact@v4
        with:
          name: archive-metadata
          path: ./
      
      - name: ğŸ” Validate Metadata
        id: validate
        run: |
          echo "ğŸ” Validating data integrity..."
          
          if [ ! -f "archive-metadata.json" ]; then
            echo "âŒ Metadata file not found!"
            echo "passed=false" >> $GITHUB_OUTPUT
            exit 1
          fi
          
          # Parse metadata with null handling
          TOTAL=$(jq -r '.total_entries // 0' archive-metadata.json)
          PREV=$(jq -r '.previous_entries // 0' archive-metadata.json)
          NEW=$(jq -r '.new_entries // 0' archive-metadata.json)
          HAS_PREV=$(jq -r '.has_previous_data // "false"' archive-metadata.json)
          
          echo "ğŸ“Š Data Statistics:"
          echo "  â€¢ Previous entries: $PREV"
          echo "  â€¢ New entries: $NEW"
          echo "  â€¢ Total entries: $TOTAL"
          
          HAS_WARNINGS=false
          
          # Check for data loss
          if [ "$HAS_PREV" == "true" ]; then
            # Use arithmetic comparison with error suppression
            if [ "$TOTAL" -lt "$PREV" ] 2>/dev/null; then
              LOST=$((PREV - TOTAL))
              echo "::error::âš ï¸ DATA LOSS DETECTED! Lost $LOST entries"
              echo "::error::This may indicate a scraping failure or data corruption"
              HAS_WARNINGS=true
              
              # Create warning file for summary
              cat > validation-warning.txt << EOF
          ## âš ï¸ DATA LOSS DETECTED
          
          **Previous entries:** $PREV  
          **Current total:** $TOTAL  
          **Missing entries:** $LOST
          
          Please investigate before using this release!
          EOF
            fi
          fi
          
          # Check if new scrape yielded data
          if [ "$NEW" -eq 0 ] 2>/dev/null; then
            echo "::warning::No new data collected in this run"
            HAS_WARNINGS=true
          fi
          
          # Validation passes if we have data and no critical errors
          if [ "$TOTAL" -gt 0 ] 2>/dev/null; then
            echo "passed=true" >> $GITHUB_OUTPUT
            echo "has_warnings=$HAS_WARNINGS" >> $GITHUB_OUTPUT
            echo "âœ… Validation passed"
          else
            echo "passed=false" >> $GITHUB_OUTPUT
            echo "has_warnings=true" >> $GITHUB_OUTPUT
            echo "âŒ Validation failed: No data in archive"
            exit 1
          fi
      
      - name: ğŸ“Š Generate Validation Summary
        if: always()
        run: |
          echo "## âœ… Data Validation Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f "validation-warning.txt" ]; then
            cat validation-warning.txt >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ "${{ steps.validate.outputs.passed }}" == "true" ]; then
            echo "âœ… **Status:** Passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ **Status:** Failed" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f "archive-metadata.json" ]; then
            TOTAL=$(jq -r '.total_entries' archive-metadata.json)
            PREV=$(jq -r '.previous_entries' archive-metadata.json)
            NEW=$(jq -r '.new_entries' archive-metadata.json)
            
            echo "### ğŸ“Š Entry Counts" >> $GITHUB_STEP_SUMMARY
            echo "- Previous: $PREV" >> $GITHUB_STEP_SUMMARY
            echo "- New: $NEW" >> $GITHUB_STEP_SUMMARY
            echo "- Total: $TOTAL" >> $GITHUB_STEP_SUMMARY
          fi
  
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # PHASE 4: UPLOAD (Publish to Releases)
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  
  upload-to-release:
    name: ğŸ“¤ Upload to GitHub Release
    if: ${{ github.event.inputs.skip_upload != 'true' }}
    needs: 
      - compress-data
      - validate-data
    permissions:
      contents: write
    uses: ./.github/workflows/upload-release.yml
    secrets: inherit
    with:
      max_backups: ${{ github.event.inputs.max_backups && fromJSON(github.event.inputs.max_backups) || 30 }}
  
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # FINAL: NOTIFICATION & SUMMARY
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  
  finalize:
    name: âœ… Pipeline Complete
    needs:
      - download-previous
      - scrape-steamspy
      - compress-data
      - validate-data
      - upload-to-release
    runs-on: ubuntu-latest
    if: always()
    
    steps:
      - name: ğŸ“Š Generate Final Summary
        run: |
          echo "# ğŸ‰ Pipeline Execution Complete!" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Job statuses
          echo "## ğŸ“‹ Job Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Phase | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Download Previous | ${{ needs.download-previous.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Scrape SteamSpy | ${{ needs.scrape-steamspy.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Compress Data | ${{ needs.compress-data.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Validate Data | ${{ needs.validate-data.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Upload Release | ${{ needs.upload-to-release.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Data statistics
          if [ "${{ needs.compress-data.outputs.total_entries }}" != "" ]; then
            echo "## ğŸ“Š Data Statistics" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "- **Total Entries:** ${{ needs.compress-data.outputs.total_entries }}" >> $GITHUB_STEP_SUMMARY
            echo "- **Archive Size:** ${{ needs.compress-data.outputs.archive_size }}" >> $GITHUB_STEP_SUMMARY
            echo "- **Compression Ratio:** ${{ needs.compress-data.outputs.compression_ratio }}" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Warnings
          if [ "${{ needs.validate-data.outputs.has_warnings }}" == "true" ]; then
            echo "## âš ï¸ Warnings" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Data validation completed with warnings. Please review the validation step above." >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "**Timestamp:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
          echo "**Workflow Run:** [View Details](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> $GITHUB_STEP_SUMMARY
      
      - name: ğŸŠ Success Notification
        if: success()
        run: |
          echo "âœ… All pipeline tasks completed successfully!"
          echo "ğŸ“¦ Data has been collected, validated, compressed, and uploaded."
          echo "ğŸ”— Check releases for the latest data snapshot."
      
      - name: âŒ Failure Notification
        if: failure()
        run: |
          echo "âŒ Pipeline execution failed!"
          echo "Please check the failed jobs above for details."
          exit 1